{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:50:31.668148Z",
     "start_time": "2023-11-19T01:50:31.665645Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lifelines import CoxPHFitter\n",
    "import shap\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as st\n",
    "from bayes_opt import BayesianOptimization\n",
    "from pyirr import intraclass_correlation\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from feature_utils.feature_classfication import data_normalization, data_normalization_apply_cohort1_to_all, lgb_evaluate_lgbm, lgb_evaluate_svm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:56:43.983412Z",
     "start_time": "2023-11-19T01:56:43.976633Z"
    }
   },
   "id": "17061ec3283676d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/wangpengcheng/Documents/qsu_data/sub_all_n4b_c.csv')\n",
    "data = data[(data['Subtypes_three']==0)|(data['Subtypes_three']==1)|(data['Subtypes_three']==2)]\n",
    "id_all = data[(data['Cohort']==1)]['id']\n",
    "id_ex = data[(data['Cohort']!=1)]['id']\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    if data.loc[i, 'Sex'] == 'f' or data.loc[i, 'Sex'] == 'F':\n",
    "        data.loc[i, 'Sex'] = 0\n",
    "    else:\n",
    "        data.loc[i, 'Sex'] = 1\n",
    "        \n",
    "# delete rows with nan\n",
    "data = data.dropna(axis=0, subset=['T2_diagnostics_Image-original_Mean_x'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "label = data['Subtypes_three']\n",
    "cohort = data['Cohort']\n",
    "age = data['Age']\n",
    "\n",
    "data = data.iloc[:, np.r_[8:7896]]\n",
    "data = data.dropna(axis=1,how='any')\n",
    "\n",
    "for col in data.columns:\n",
    "    try:\n",
    "        data[col] = data[col].astype(float)\n",
    "    except:\n",
    "        del data[col]\n",
    "\n",
    "data = data_normalization(data)\n",
    "data = pd.concat([cohort, label, age, data], axis=1, ignore_index=False)\n",
    "\n",
    "print(\"subjects: {}, features: {}\".format(data.shape[0],data.shape[1]-2))\n",
    "print(\"Head 8 columns name:\", data.columns[:8])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86da9fd598a3f109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do feature selection\n",
    "from feature_utils.feature_selection import feature_selection_rf, feature_selection_auc\n",
    "from collections import Counter\n",
    "\n",
    "cohort1_data = data[data['Cohort']==1].iloc[:, 2:]\n",
    "cohort1_label = data[data['Cohort']==1].iloc[:, 1]\n",
    "\n",
    "# random forest selection\n",
    "features_all_sel = []\n",
    "for m in tqdm(range(100)):\n",
    "    sel_data = feature_selection_rf(cohort1_data, cohort1_label, feature_num=20)\n",
    "    features_all_sel = features_all_sel + sel_data.columns.tolist()\n",
    "print(Counter(features_all_sel))\n",
    "\n",
    "# AUC selection\n",
    "sel_data = feature_selection_auc(cohort1_data, cohort1_label, range_auc=[0.42, 0.58])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a470142e1b9a3b85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# selected features\n",
    "wnt_shh_g34_feature_names = ['T1E_log-sigma-3-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis_x',\n",
    " 'T1E_log-sigma-3-mm-3D_glcm_Autocorrelation_x',\n",
    " 'T1E_log-sigma-1-mm-3D_glcm_ClusterShade_x',\n",
    " 'T1E_wavelet-LHL_firstorder_Mean_x',\n",
    " 'T2_wavelet-LLH_glcm_ClusterShade_y',\n",
    " 'T1E_log-sigma-5-mm-3D_glrlm_LowGrayLevelRunEmphasis_x',\n",
    " 'T1E_wavelet-HLL_firstorder_Median_x',\n",
    " 'T1E_wavelet-LHL_firstorder_Median_x',\n",
    " 'T2_wavelet-HHL_firstorder_Skewness_x',\n",
    " 'T1E_log-sigma-3-mm-3D_glrlm_LongRunLowGrayLevelEmphasis_x',\n",
    " 'T1E_lbp-2D_glrlm_RunLengthNonUniformity_x',\n",
    " 'T2_log-sigma-1-mm-3D_gldm_LowGrayLevelEmphasis_x',\n",
    " 'T1E_log-sigma-5-mm-3D_firstorder_90Percentile_x',\n",
    " 'T1E_log-sigma-3-mm-3D_glszm_LowGrayLevelZoneEmphasis_x',\n",
    " 'T2_wavelet-HHL_glcm_SumSquares_x',\n",
    " 'T2_wavelet-HHL_glrlm_GrayLevelNonUniformityNormalized_x',\n",
    " 'T2_wavelet-LLH_gldm_DependenceNonUniformity_x',\n",
    " 'T2_original_shape_Maximum3DDiameter_y',\n",
    " 'T2_original_glszm_LargeAreaEmphasis_y']\n",
    "\n",
    "sel_columns = ['Cohort', 'Subtypes_three', 'Age'] + wnt_shh_g34_feature_names\n",
    "\n",
    "x = data[sel_columns]\n",
    "x.iloc[:, :] = data_normalization_apply_cohort1_to_all(x.iloc[:, :])\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9708dba98c1554e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split base dataset and external validation dataset\n",
    "df_val = x[x['Cohort'] != 1]\n",
    "df_val.reset_index(inplace=True)\n",
    "X_val = df_val.iloc[:, 3:]\n",
    "\n",
    "feature_names = X_val.columns.tolist()\n",
    "X_val = np.array(X_val)\n",
    "y_val = df_val['Subtypes_three']\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print('X-val: ', X_val.shape)\n",
    "      \n",
    "df_X = x[x['Cohort'] == 1]\n",
    "df_X.reset_index(inplace=True)\n",
    "X = df_X.iloc[:, 3:]\n",
    "y = df_X['Subtypes_three']\n",
    "print('X: ', X.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a6dd725f6fc7b7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and find the best hyperparameters for lgbm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def f1_scores(label_f1, pred_f1, pos_index):\n",
    "    \"\"\"\n",
    "    Calculate f1 score（for bootstrap）\n",
    "    :param data: data[:, 0] = pred, data[:, 1] = label, data[:, 2] = index\n",
    "    :return: f1 score\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(y_true=label_f1, y_pred=pred_f1)\n",
    "\n",
    "    tp = conf_mat[pos_index][pos_index]\n",
    "    fp = np.sum(conf_mat[:, pos_index]) - tp\n",
    "    fn = np.sum(conf_mat[pos_index]) - tp\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_cls = 2 * precision * recall / (precision + recall)\n",
    "    return f1_cls\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=32)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_test = np.array(y_test).astype(int)\n",
    "    \n",
    "    nm = SMOTE(random_state=42)\n",
    "    X_res, y_res = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('size of data partitions')\n",
    "    print('...............INPUT..............OUTPUT..........')\n",
    "    print('Train      : ', X_train.shape, '         ', y_train.shape, np.sum(y_train))\n",
    "    print('Test       : ', X_test.shape, '         ', y_test.shape, np.sum(y_test))\n",
    "    print('Validation : ', X_val.shape, '         ', y_val.shape, np.sum(y_val))\n",
    "\n",
    "    # bayesian optimization of hyperparameters for lg_boost\n",
    "    gp_params = {\"alpha\": 1e-4}\n",
    "    seed = 1\n",
    "\n",
    "    # use partial to pass train, val and test data\n",
    "    partial_lgb_evaluate_lgbm = partial(lgb_evaluate_lgbm, X_train=X_res, y_train=y_res, X_val=X_val, y_val=y_val,\n",
    "                                        X_test=X_test, y_test=y_test, seed=1, multi_cls=True)\n",
    "    lgb_bo = BayesianOptimization(partial_lgb_evaluate_lgbm, {'max_depth': (1, 10),\n",
    "                                                 'num_leaves': (2,10),\n",
    "                                                 'learning_rate':(0.01, 0.4),\n",
    "                                                 'max_bin':(200, 800),\n",
    "                                                 'colsample_bytree': (0.01, 1.0),\n",
    "                                                 'reg_alpha':(1,10),\n",
    "                                                 'reg_lambda':(0.01,1),\n",
    "                                                 'subsample':(0.01,1),\n",
    "                                                 'min_child_samples' : (5, 30),\n",
    "                                                 'min_child_weight':(1, 5),\n",
    "                                                 'n_estimators' :(500,2200),\n",
    "                                                 'scale_pos_weight': (1,7),})\n",
    "\n",
    "    # Optimally needs quite a few more initiation points and number of iterations\n",
    "    lgb_bo.maximize(init_points=200, n_iter=50, acq='ei',  **gp_params)\n",
    "\n",
    "    print(lgb_bo.max)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7323b518de5b837"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# test and evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=32)\n",
    "test_folds = []\n",
    "val_folds = []\n",
    "true_folds = []\n",
    "pred_folds = []\n",
    "cm_folds = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_test = np.array(y_test).astype(int)\n",
    "\n",
    "    nm = SMOTE(random_state=42)\n",
    "    X_res, y_res = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "    pa1 = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.17866332260414736,\n",
    "           'importance_type': 'split', 'learning_rate': 0.31663818808466854, 'max_depth': 3,\n",
    "           'min_child_samples': 20, 'min_child_weight': 3.7045345974251696, 'min_split_gain': 0.0,\n",
    "           'n_estimators': 1191, 'n_jobs': -1, 'num_leaves': 6, 'objective': None, 'random_state': 1,\n",
    "           'reg_alpha': 2.7808856311910626, 'reg_lambda': 0.43654141243810257, 'silent': True,\n",
    "           'subsample': 0.7965933438188254, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'max_bin': 323,\n",
    "           'scale_pos_weight': 3.7269851872292934}\n",
    "    pa2 = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.03379242822507714,\n",
    "           'importance_type': 'split', 'learning_rate': 0.12613026534842414, 'max_depth': 1,\n",
    "           'min_child_samples': 16, 'min_child_weight': 4.163032273614103, 'min_split_gain': 0.0,\n",
    "           'n_estimators': 616, 'n_jobs': -1, 'num_leaves': 7, 'objective': None, 'random_state': 1,\n",
    "           'reg_alpha': 1.8283478827688406, 'reg_lambda': 0.2069651728126041, 'silent': True,\n",
    "           'subsample': 0.7005502102578648, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'max_bin': 437,\n",
    "           'scale_pos_weight': 2.432742203577984}\n",
    "    pa3 = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.41926099496682023,\n",
    "           'importance_type': 'split', 'learning_rate': 0.29289472226649155, 'max_depth': 9,\n",
    "           'min_child_samples': 12, 'min_child_weight': 3.564392827408104, 'min_split_gain': 0.0,\n",
    "           'n_estimators': 1004, 'n_jobs': -1, 'num_leaves': 8, 'objective': None, 'random_state': 1,\n",
    "           'reg_alpha': 1.5238007376941431, 'reg_lambda': 0.2882531480596396, 'silent': True,\n",
    "           'subsample': 0.9738277910821249, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'max_bin': 617,\n",
    "           'scale_pos_weight': 1.5855218660728612}\n",
    "    params_3fold_res = [pa1, pa2, pa3]\n",
    "\n",
    "    params_cur = params_3fold_res[i]\n",
    "    modellgb = LGBMClassifier()\n",
    "    modellgb.set_params(**params_cur)\n",
    "    clf_lg = modellgb.fit(X_res, y_res)\n",
    "    p_lg_train = clf_lg.predict_proba(X_res)  # [:,1]\n",
    "    p_lg_test = clf_lg.predict_proba(X_test)  # [:,1]\n",
    "    p_lg_val = clf_lg.predict_proba(X_val)  # [:,1]\n",
    "    val_folds.append(p_lg_val)\n",
    "    test_folds.append(p_lg_test)\n",
    "\n",
    "    l_lg_train = clf_lg.predict(X_res)\n",
    "    l_lg_test = clf_lg.predict(X_test)\n",
    "    l_lg_val = clf_lg.predict(X_val)\n",
    "\n",
    "    f1_train = f1_score(y_res, l_lg_train, average='weighted')\n",
    "    f1_test = f1_score(y_test, l_lg_test, average='weighted')\n",
    "    f1_val = f1_score(y_val, l_lg_val, average='weighted')\n",
    "\n",
    "    true_folds.append(y_test)\n",
    "    pred_folds.append(l_lg_test)\n",
    "\n",
    "    lg_auc_train = roc_auc_score(y_res, p_lg_train, multi_class='ovr')\n",
    "    lg_auc_test = roc_auc_score(y_test, p_lg_test, multi_class='ovr')\n",
    "    lg_auc_val = roc_auc_score(y_val, p_lg_val, multi_class='ovr')\n",
    "\n",
    "    cm_test = confusion_matrix(y_test, l_lg_test)\n",
    "    cm_folds.append(cm_test)\n",
    "\n",
    "all_folds = np.stack(val_folds)\n",
    "mean_folds = np.mean(all_folds, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:29:51.426316Z",
     "start_time": "2023-11-19T03:29:50.419586Z"
    }
   },
   "id": "7df47dcf36b3d8f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cm = np.sum(np.stack(cm_folds), axis=0)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp1.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f0b124a396f492"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_folds = np.stack(val_folds)\n",
    "mean_folds = np.mean(all_folds, axis=0)\n",
    "lg_auc_val = roc_auc_score(y_val, mean_folds, multi_class='ovr')\n",
    "print('Validation AUC : ', np.round(lg_auc_val, decimals=4))\n",
    "cm_val = confusion_matrix(y_val, np.argmax(mean_folds, axis=1))\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_val)\n",
    "disp1.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9baca5b35bcbacf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1af51fb8bd809ac9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
